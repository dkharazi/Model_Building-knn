---
title: "Income"
author: "Darius Kharazi"
date: "9/23/2017"
output: pdf_document
---

## Import Statements

```{r import, include=FALSE}
## Import Libraries 
library(knitr)
library(readr)
library(class)
library(pROC)
library(ggplot2)
library(ROCR)

## Import Dataset
income <- read_csv("~/Downloads/income/income_tr.csv") # Change to location of "income_tr.csv"
income.test <- read_csv("~/Downloads/income/income_te.csv") # Change to location of "income_te.csv"
income.df <- data.frame(income)
income.test.df <- data.frame(income.test)

## Create new data frame
newincome.df <- data.frame(income.df$ID)
newincome.df[,2:43] <- 0
newincome.test.df <- data.frame(income.test.df$ID)
newincome.test.df[,2:43] <- 0

## Rename columns for training and test
colnames(newincome.df) <- c("ID", "isAge10-19", "isAge20-29", "isAge30-39", "isAge40-49", "isAge50-59", "isAge60-69", "isAge70-79", "isAge80-89", "isWorkPriv", "isWorkOther", "isWgt20-99k", "isWgt100-199k", "isWgt200-299k", "isWgt300-399k", "isWgt400-499k", "isWgt500k+", "isEduHighGradOrLess", "isEduSCol-Bach", "isEduMastOrMore", "isMarried", "isNeverMarried", "isDivOrSep", "isWidowed", "isOccBus", "isOccTech", "isOccOther", "isRelHusbOrWife", "isRelUnmarried", "isRelOther", "isRaceWhite", "isRaceOther", "isMale", "isFemale", "isCapGain0", "isCapGainOther", "isCapLoss0", "isCapLossOther", "isHPW40OrLess", "isHPW41OrMore", "isCountryUS", "isCountryOther", "isClass>50K")

colnames(newincome.test.df) <- c("ID", "isAge10-19", "isAge20-29", "isAge30-39", "isAge40-49", "isAge50-59", "isAge60-69", "isAge70-79", "isAge80-89", "isWorkPriv", "isWorkOther", "isWgt20-99k", "isWgt100-199k", "isWgt200-299k", "isWgt300-399k", "isWgt400-499k", "isWgt500k+", "isEduHighGradOrLess", "isEduSCol-Bach", "isEduMastOrMore", "isMarried", "isNeverMarried", "isDivOrSep", "isWidowed", "isOccBus", "isOccTech", "isOccOther", "isRelHusbOrWife", "isRelUnmarried", "isRelOther", "isRaceWhite", "isRaceOther", "isMale", "isFemale", "isCapGain0", "isCapGainOther", "isCapLoss0", "isCapLossOther", "isHPW40OrLess", "isHPW41OrMore", "isCountryUS", "isCountryOther", "isClass>50K")
```

## Preprocessing

```{r class}
## Get indices of corresponding entries for "class"
classInd <- which(income.df$class == ">50K")
classInd.test <- which(income.test.df$class == ">50K")

## Assign correct values to newincome.df according to "class"
newincome.df$`isClass>50K`[classInd] = 1
newincome.test.df$`isClass>50K`[classInd.test] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isClass>50K if the individual is a part of the class that is greater than 50k;
- Otherwise, the values will be 0.

After loading in the required libraries and data frames, we want to factor some of the variables into more appropriate categories.

```{r age, include=FALSE}
## Get indices of corresponding entries for "age"
tenInd <- which(income.df$age >= 10 & income.df$age <= 19)
twentyInd <- which(income.df$age >= 20 & income.df$age <= 29)
thirtyInd <- which(income.df$age >= 30 & income.df$age <= 39)
fortyInd <- which(income.df$age >= 40 & income.df$age <= 49)
fiftyInd <- which(income.df$age >= 50 & income.df$age <= 59)
sixtyInd <- which(income.df$age >= 60 & income.df$age <= 69)
seventyInd <- which(income.df$age >= 70 & income.df$age <= 79)
eightyInd <- which(income.df$age >= 80 & income.df$age <= 89)

## Assign correct values to newincome.df according to "age"
newincome.df$`isAge10-19`[tenInd] = 1
newincome.df$`isAge20-29`[twentyInd] = 1
newincome.df$`isAge30-39`[thirtyInd] = 1
newincome.df$`isAge40-49`[fortyInd] = 1
newincome.df$`isAge50-59`[fiftyInd] = 1
newincome.df$`isAge60-69`[sixtyInd] = 1
newincome.df$`isAge70-79`[seventyInd] = 1
newincome.df$`isAge80-89`[eightyInd] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isAge10-19 if the individual is between the ages of 10 and 19 (inclusive);  
- isAge20-29 if the individual is between the ages of 20 and 29 (inclusive);  
- isAge30-39 if the individual is between the ages of 30 and 39 (inclusive);  
- isAge40-49 if the individual is between the ages of 40 and 49 (inclusive);  
- isAge50-59 if the individual is between the ages of 50 and 59 (inclusive);  
- isAge60-69 if the individual is between the ages of 60 and 69 (inclusive);  
- isAge70-79 if the individual is between the ages of 70 and 79 (inclusive);  
- isAge80-89 if the individual is between the ages of 80 and 89 (inclusive);  
- Otherwise, the values will be 0.

The question is whether recoding missing values into valid 0 is warranted. For binary (0 vs 1) data 0 means "absent" while missing means "absent or present - not known". No similarity measure itself can help you decide how to go about missing data.

```{r workclass, include=FALSE}
## Get indices of corresponding entries for "workclass"
privInd <- which(income.df$workclass %in% c("Private"))
othInd <- which(!income.df$workclass %in% c("Private"))

## Assign correct values to newincome.df according to "workclass"
newincome.df$isWorkPriv[privInd] = 1
newincome.df$isWorkOther[othInd] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isWorkPriv if the individual's workclass is private;
- isWorkOther if the individual's workclass is not private.
- Otherwise, the values will be 0.

```{r wgt, include=FALSE}
## Get indices of corresponding entries for "fnlwgt"
twentyInd <- which(income.df$fnlwgt >= 20000 & income.df$fnlwgt < 100000)
onehundInd <- which(income.df$fnlwgt >= 100000 & income.df$fnlwgt < 200000)
twohundInd <- which(income.df$fnlwgt >= 200000 & income.df$fnlwgt < 300000)
threehundInd <- which(income.df$fnlwgt >= 300000 & income.df$fnlwgt < 400000)
fourhundInd <- which(income.df$fnlwgt >= 400000 & income.df$fnlwgt < 500000)
fivehundInd <- which(income.df$fnlwgt >= 500000 & income.df$fnlwgt < 1000000)

## Assign correct values to newincome.df according to "fnlwgt"
newincome.df$`isWgt20-99k`[twentyInd] = 1
newincome.df$`isWgt100-199k`[onehundInd] = 1
newincome.df$`isWgt200-299k`[twohundInd] = 1
newincome.df$`isWgt300-399k`[threehundInd] = 1
newincome.df$`isWgt400-499k`[fourhundInd] = 1
newincome.df$`isWgt500k+`[fivehundInd] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isWft20-99k if the individual has a "fnlwgt" between 20,000 and 100,000 (exclusive);
- isWft100-199k if the individual has a "fnlwgt" between 100,000 and 200,000 (exclusive);
- isWft200-299k if the individual has a "fnlwgt" between 200,000 and 300,000 (exclusive);
- isWft300-399k if the individual has a "fnlwgt" between 300,000 and 400,000 (exclusive);
- isWft400-499k if the individual has a "fnlwgt" between 400,000 and 500,000 (exclusive);
- isWft500k+ if the individual has a "fnlwgt" of 500,000 or greater.
- Otherwise, the values will be 0.

```{r education, include=FALSE}
## Get indices of corresponding entries for "education"
highInd <- which(income.df$education %in% c("Preschool", "1st-4th", "5th-6th", "7th-8th", 
                                            "9th", "10th", "11th", "12th", "HS-grad"))
bachInd <- which(income.df$education %in% c("Some-college", "Assoc-acdm", "Assoc-voc",
                                             "Bachelors"))
mastInd <- which(income.df$education %in% c("Masters", "Doctorate", "Prof-school"))

## Assign correct values to newincome.df according to "education"
newincome.df$isEduHighGradOrLess[highInd] = 1
newincome.df$`isEduSCol-Bach`[bachInd] = 1
newincome.df$isEduMastOrMore[mastInd] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isEduHighGradOrLess if the individual has a high school education or less;
- isEduSCol-Bach if the individual has education between some college and a bachelors degree;
- isEduMastOrMore if the individual has a Masters degree or more.
- Otherwise, the values will be 0.

Also, the variable "education_cat" was excluded from the newly created data frame, since the variable is essentially equivalent to the "education" variable in the "income.df" data frame.

```{r maritalstatus, include=FALSE}
## Get indices of corresponding entries for "marital_status"
marriedInd <- which(income.df$marital_status %in% c("Married-AF-spouse", 
                                                    "Married-civ-spouse", 
                                                    "Married-spouse-absent"))
nMarriedInd <- which(income.df$marital_status %in% c("Never-married"))
divInd <- which(income.df$marital_status %in% c("Divorced", "Separated"))
widInd <- which(income.df$marital_status %in% c("Widowed"))

## Assign correct values to newincome.df according to "marital_status"
newincome.df$isMarried[marriedInd] = 1
newincome.df$isNeverMarried[nMarriedInd] = 1
newincome.df$isDivOrSep[divInd] = 1
newincome.df$isWidowed[widInd] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isMarried if the individual is married;
- isNeverMarried if the individual has never been married;
- isDivOrSep if the individual is divorced or separated;
- isWidowed if the individual is a widow.
- Otherwise, the values will be 0.

```{r occupation, include=FALSE}
## Get indices of corresponding entries for "occupation"
occBusInd <- which(income.df$occupation %in% c("Adm-clerical", 
                                                    "Exec-managerial", 
                                                    "Sales"))
occTechInd <- which(income.df$occupation %in% c("Machine-op-inspct", "Tech-support"))
occOthInd <- which(income.df$occupation %in% c("?", "Craft-repair", "Farming-fishing",
                                               "Handlers-cleaners", "Other-service", 
                                               "Priv-house-serv", "Prof-specialty", 
                                               "Protective-serv", "Transport-moving"))

## Assign correct values to newincome.df according to "occupation"
newincome.df$isOccBus[occBusInd] = 1
newincome.df$isOccTech[occTechInd] = 1
newincome.df$isOccOther[occOthInd] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isOccBus if the individual has an occupation relating to business;
- isOccTech if the individual has an occupation relating to technology;
- isOccOther if the individual has an occupation that doesn't relate to business or technology.
- Otherwise, the values will be 0.


```{r relationship, include=FALSE}
## Get indices of corresponding entries for "relationship"
relHW <- which(income.df$relationship %in% c("Husband", 
                                                    "Wife"))
relUnmarr <- which(income.df$relationship %in% c("Unmarried"))
relOther <- which(income.df$relationship %in% c("Not-in-family", "Other-relative",
                                                "Own-child"))

## Assign correct values to newincome.df according to "relationship"
newincome.df$isRelHusbOrWife[relHW] = 1
newincome.df$isRelUnmarried[relUnmarr] = 1
newincome.df$isRelOther[relOther] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isRelHusbOrWife if the individual either considers themselves as a husband or wife;
- isRelUnmarried if the individual is unmarried;
- isRelOther if the individual doesn't consider themselves as married or unmarried.
- Otherwise, the values will be 0.

```{r race, include=FALSE}
## Get indices of corresponding entries for "race"
raceWhite <- which(income.df$race %in% c("White"))
raceOther <- which(!income.df$race %in% c("White"))

## Assign correct values to newincome.df according to "race"
newincome.df$isRaceWhite[raceWhite] = 1
newincome.df$isRaceOther[raceOther] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isRaceWhite if the individual is white;
- isRaceOther if the individual is not white.
- Otherwise, the values will be 0.

```{r gender, include=FALSE}
## Get indices of corresponding entries for "gender"
genMale <- which(income.df$gender %in% c("Male"))
genFemale <- which(income.df$gender %in% c("Female"))

## Assign correct values to newincome.df according to "gender"
newincome.df$isMale[genMale] = 1
newincome.df$isFemale[genFemale] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isMale if the individual is male;
- isFemale if the individual is female.
- Otherwise, the values will be 0.

```{r capgain, include=FALSE}
## Get indices of corresponding entries for "capital_gain"
gainZero <- which(income.df$capital_gain == 0)
gainOther <- which(income.df$capital_gain != 0)

## Assign correct values to newincome.df according to "capital_gain"
newincome.df$isCapGain0[gainZero] = 1
newincome.df$isCapGainOther[gainOther] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isCapGain0 if the individual has a capital gain that equals 0;
- isCapGainOther if the individual has a capital gain that is greater than 0.
- Otherwise, the values will be 0.

```{r caploss, include=FALSE}
## Get indices of corresponding entries for "capital_loss"
lossZero <- which(income.df$capital_loss == 0)
lossOther <- which(income.df$capital_loss != 0)

## Assign correct values to newincome.df according to "capital_loss"
newincome.df$isCapLoss0[lossZero] = 1
newincome.df$isCapLossOther[lossOther] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isCapLoss0 if the individual has a capital loss that equals 0;
- isCapLossOther if the individual has a capital loss that is greater than 0.
- Otherwise, the values will be 0.

```{r hrsperweek, include=FALSE}
## Get indices of corresponding entries for "hours_per_week"
hrsForty <- which(income.df$hour_per_week > 0 & income.df$hour_per_week <= 40)
hrsGreater <- which(income.df$hour_per_week > 40 & income.df$hour_per_week <= 100)

## Assign correct values to newincome.df according to "hours_per_week"
newincome.df$isHPW40OrLess[hrsForty] = 1
newincome.df$isHPW41OrMore[hrsGreater] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isHPW40OrLess if the individual works 40 hours per week or less;
- isHPW41OrMore if the individual works 41 hours per week or more.
- Otherwise, the values will be 0.

```{r country, include=FALSE}
## Get indices of corresponding entries for "native_country"
countryUS <- which(income.df$native_country %in% c("United-States"))
countryOther <- which(!income.df$native_country %in% c("United-States"))

## Assign correct values to newincome.df according to "native_country"
newincome.df$isCountryUS[countryUS] = 1
newincome.df$isCountryOther[countryOther] = 1
```

The newly created data frame will be assigned the value 1 to each of the following variables:

- isCountryUS if the individual's native country is the United States;
- isCountryOther if the individual's native country is anywhere that isn't the United States.
- Otherwise, the values will be 0.

## Jaccard Similarity

```{r jaccard, include=FALSE}
## Convert data frame to matrix
newincome.m <- data.matrix(newincome.df)
newincome.test.m <- data.matrix(newincome.test.df)

## Create adjustable k
k <- 10 # Adjust k here

## Create a new data frame containing jaccard similarities and output values
jaccard.m <- matrix(nrow=(nrow(newincome.df)+1), ncol=(nrow(newincome.df)+1))
jaccard.m[2:nrow(jaccard.m),1] <- newincome.df$ID
jaccard.m[1,2:nrow(jaccard.m)] <- newincome.df$ID
jaccard.df <- data.frame(income.df$ID)
jaccard.df[,2:((k*2)+1)] <- 0

jaccard.test.m <- matrix(nrow=(nrow(newincome.df)+1), ncol=(nrow(newincome.test.df)+1))
jaccard.test.m[2:nrow(jaccard.test.m),1] <- newincome.df$ID
jaccard.test.m[1,2:ncol(jaccard.test.m)] <- newincome.test.df$ID
jaccard.test.df <- data.frame(income.test.df$ID)
jaccard.test.df[,2:((k*2)+1)] <- 0

## Rename columns
names(jaccard.df)[seq(2,ncol(jaccard.df),2)] <- "Index"
names(jaccard.df)[seq(1,ncol(jaccard.df),2)] <- "Proximity"
names(jaccard.df)[1] <- "ID"

names(jaccard.test.df)[seq(2,ncol(jaccard.test.df),2)] <- "Index"
names(jaccard.test.df)[seq(1,ncol(jaccard.test.df),2)] <- "Proximity"
names(jaccard.test.df)[1] <- "ID"

## Create a function for calculating euclidean similarity of two distinct data entries
jaccard <- function(indi, indj) {
  bmask <- rep(1, 41)
  xVec <- as.numeric(newincome.df[indi,2:42])
  yVec <- as.numeric(newincome.df[indj,2:42])
  M01 <- sum((!xVec&bmask)&(yVec&bmask))
  M10 <- sum((xVec&bmask)&(!yVec&bmask))
  M00 <- sum(!xVec & !yVec)
  M11 <- sum(xVec & yVec)
  simJ <- (M11)/(M01+M10+M11)
  return(simJ)
}

## Create a function for calculating cosine similarity of two distinct data entries
cosine <- function(indi, indj) {
  xVec <- as.numeric(newincome.df[indi,2:42])
  yVec <- as.numeric(newincome.df[indj,2:42])
  xlen <- sqrt(sum(xVec^2))
  ylen <- sqrt(sum(yVec^2))
  simC <- sum(t(xVec)*yVec)/(xlen*ylen)
  return(simC)
}

jaccard.test <- function(indi, indj) {
  bmask <- rep(1, 41)
  xVec <- as.numeric(newincome.df[indi,2:42])
  yVec <- as.numeric(newincome.test.df[indj,2:42])
  M01 <- sum((!xVec&bmask)&(yVec&bmask))
  M10 <- sum((xVec&bmask)&(!yVec&bmask))
  M00 <- sum(!xVec & !yVec)
  M11 <- sum(xVec & yVec) 
  simJ <- (M11)/(M01+M10+M11)
  return(simJ)
}

## Perform function for each entry
for (rowi in 1:nrow(newincome.df)) {
  for (rowj in 1:nrow(newincome.df)) {
    jaccard.m[rowi+1, rowj+1] <- jaccard(rowi, rowj) # Adjust function by changing from jaccard function to cosine
  }
}

for (rowi in 1:nrow(newincome.test.df)) {
  for (rowj in 1:nrow(newincome.test.df)) {
    jaccard.test.m[rowi+1, rowj+1] <- jaccard.test(rowi, rowj)
  }
}

## Convert matrix to dataframe
jacsim.df <- data.frame(jaccard.m)
jacsim.df[1,1] = 0
colnames(jacsim.df) <- jacsim.df[1,]
jacsim.df <- jacsim.df[-1,]
colnames(jacsim.df)[1] <- "ID"

jacsim.test.df <- data.frame(jaccard.test.m)
jacsim.test.df[1,1] = 0
colnames(jacsim.test.df) <- jacsim.test.df[1,]
jacsim.test.df <- jacsim.test.df[-1,]
colnames(jacsim.test.df)[1] <- "ID"

## Remove high similarities of entries similar to itself
for(i in 1:nrow(jacsim.df)) {
  jacsim.df[i, i+1] = 0
}

## Find k most similar entries and add values from matrix to output df
for(col in 2:ncol(jacsim.df)) {
  i = 0
  kmost <- sort(jacsim.df[,col], index.return=TRUE, decreasing = TRUE)
  kmost <- lapply(kmost, `[`, kmost$x %in% head(kmost$x,k))
  for(coscol in 1:k) {
    jaccard.df[col-1,coscol+i+1] <- kmost$ix[coscol]
    jaccard.df[col-1,coscol+i+2] <- kmost$x[coscol]
    i = i + 1
  }
}

for(col in 2:ncol(jacsim.test.df)) {
  i = 0
  kmost <- sort(jacsim.test.df[,col], index.return=TRUE, decreasing = TRUE)
  kmost <- lapply(kmost, `[`, kmost$x %in% head(kmost$x,k))
  for(coscol in 1:k) {
    jaccard.test.df[col-1,coscol+i+1] <- kmost$ix[coscol]
    jaccard.test.df[col-1,coscol+i+2] <- kmost$x[coscol]
    i = i + 1
  }
}
```

## Output Data Frame: Predicted vs. Actual

```{r output}
## Create output matrix
output.df <- data.frame(newincome.test.df$ID)
output.df[,2:4] <- 0

## Rename columns
colnames(output.df) <- c("ID", "Actual Class", "Predicted Class", "Posterior Probability")

## Post the Actual Class
output.df$`Actual Class` <- sapply(output.df$ID, function(x) newincome.test.df$`isClass>50K`[newincome.test.df$ID == x])

## Calculate the Posterior Probability
i <- 0
totalsum <- 0
prob <- 0
greaterfif <- 0
lessfif <- 0
for(row in 1:nrow(jaccard.test.df)) {
  for(col in 1:k) {
    if(newincome.test.df$`isClass>50K`[jaccard.test.df[row, col+i+1]] == 1) {
      greaterfif <- greaterfif + jaccard.test.df[row, col+i+2]
    } else {
      lessfif <- lessfif + jaccard.test.df[row, col+i+2]
    }
    totalsum <- totalsum + jaccard.test.df[row, col+i+2]
    i <- i + 1
  }
  prob <- greaterfif/totalsum
  if(prob > 0.5) {
    output.df$`Posterior Probability`[row] <- prob
    output.df$`Predicted Class`[row] <- 1
  } else {
    prob <- lessfif/totalsum
    output.df$`Posterior Probability`[row] <- prob
    output.df$`Predicted Class`[row] <- 0
  }
  i <- 0
  totalsum <- 0
  prob <- 0
  greaterfif <- 0
  lessfif <- 0
}

## Calculate the Error Rate
err.output <- 100 * (sum(output.df$`Actual Class` == output.df$`Predicted Class`)/nrow(output.df))
err.output

## Create a confusion matrix
table(output.df$`Actual Class`, output.df$`Predicted Class`)
```

The training data was used in the first report, which effectively discovered predictive relationships between its observations. First, exploratory analysis was conducted on the training data to understand whether the data should be transformed or not. After performing any necessary transformations and binarizing the data, jaccard similarities were calculated between each observation. The k-most similar observations were shown for each observation, which had been determined by the highest jaccard similarity values.

This report required applications from the previous report, along with an implementation of a kNN algorithm based on matching our testing data to our previously applied training data. Essentially, posterior probabilities were computed by calculating jaccard similarities from the testing data, rather than the training data. These posterior probabilities would be assigned a value depending on its degree of similarity, and provided a predicted value for each observation’s “class” variable. In order to evaluate the performance of the general classification method, these predicted “class” variables were then compared to the true “class” variables. Since the predicted “class” values were only preliminary predictions and were not perfectly comparable to the actual “class,” the true positive, false positive, true negative, and false negative values were accumulated for each posterior probability across the total amount of observations, in order to understand some of the error.

Additionally, a confusion matrix was constructed in order to understand the general accumulation of true positive, false positive, true negative, and false negative values. These true positive, false positive, true negative, and false negative values were useful for calculating True Positive rates, False Positive rates, True Negative rates, False Negative rates, Recall, Precision, and any plotting of the ROC curve.

## Compare Ouput with R Package

```{r knn}
## Initialize function input variables
train.gc <- newincome.df
test.gc <- newincome.test.df
train.def <- factor(newincome.df$`isClass>50K`)
test.def <- factor(newincome.test.df$`isClass>50K`)

## Use the knn from the Class package
knn.5 <- knn(train=train.gc, test=test.gc, cl=train.def, k=20, prob = TRUE)

## Calculate the Error Rate
err.knn <- 100 * (sum(test.def == knn.5)/nrow(test.gc))
err.knn

## Compare the error rates
err.output # using my knn function
err.knn # premade r package
```

Using the testing data was important for determining the performance of the classifiers. Moreover, adjusting the two parameters of the kNN algorithm is as important for determining the performance of the classifiers. For example, the number of neighbors k and the proximity measure impacted the performance and values given by the confusion matrix.

|       k = 5       | Predicted Success | Predicted Failure |
| ----------------- | ----------------- | ----------------- |
|  `Actual Success` |         3         |        64         |
|  `Actual Failure` |         10        |        211        |

Given that k is equal to 5, the confusion matrix for the total training data has fairly large False Negative and False Positive values. The kNN algorithm used for distinguishing the confusion matrix values is not an off-the-shelf kNN implementation, and instead a personal implementation. Additionally, the error rate for the training dataset 74.3%. Although the off-the-shelf kNN implementation is common in R, the off-the-shelf kNN implementation has comparable confusion matrix values and error rates to the personal implementation. Specifically, the off-the-shelf kNN implementation produces a similar, but worse error rate of 72.6%.

|       k = 20      | Predicted Success | Predicted Failure |
| ----------------- | ----------------- | ----------------- |
|  `Actual Success` |         0         |        67         |
|  `Actual Failure` |         0         |        211        |

Given that k is equal to 20, the confusion matrix for the total training data has a larger False Negative, a lower False Positive, a lower True Positive, and a larger True Negative. The error rate for the personal implementation is 76.7%, while the error rate for the off-the-shelf kNN implementation produces a 76.4% error rate. These error rates are nearly equivalent to each other, and essentially equal to the corresponding error rates for k equals 5. However, the values a part of the confusion matrix are differently distributed, i.e. True Positive, True Negative, False Positive, and False Negative values.

|       k = 10      | Predicted Success | Predicted Failure |
| ----------------- | ----------------- | ----------------- |
|  `Actual Success` |         0         |        67         |
|  `Actual Failure` |         4         |        217        |

Given that k is equal to 10, the confusion matrix for the total training data has a lower True Positive, larger True Negative, larger False Negative, and lower False Positive, compared to the confusion matrix for k equal to 5. The error rate for the personal implementation is 75.3%, while the error rate for the off-the-shelf kNN implementation produces a 76.7% error rate. In this case, the error rates are nearly equivalent to each other, but the error rate for the off-the-shelf implementation is the better of the two. Again, the values a part of the confusion matrix are differently distributed, which indicates the user may find each confusion matrix useful for different purposes, i.e. the lowest False Negative or lowest False Positive.

## Construct Precision, Recall, F-Score, and ROC Curve

```{r roc}
## Construct the ROC Curve Data Frame
output.df$`Posterior Probability` <- round(output.df$`Posterior Probability`, 2)
output.roc.df <- data.frame(sort(unique(output.df$`Posterior Probability`)))
output.roc.df[,2:7] <- 0
colnames(output.roc.df) <- c("Threshold", "TP", "FP", "TN", "FN", "TPR", "FPR")

## Calculate True Positive
for(thresh.ind in 1:nrow(output.roc.df)) {
  c.n <- 0
  for(post.ind in 1:nrow(output.df)) {
    if((output.roc.df$Threshold[thresh.ind]==output.df$`Posterior Probability`[post.ind]) & 
       (output.df$`Actual Class`[post.ind] == output.df$`Predicted Class`[post.ind]) &
       (output.df$`Actual Class`[post.ind] == 1)) {
      c.n <- c.n + 1
    }
  }
  output.roc.df$TP[thresh.ind] <- c.n
}

## Calculate False Positive
for(thresh.ind in 1:nrow(output.roc.df)) {
  c.n <- 0
  for(post.ind in 1:nrow(output.df)) {
    if((output.roc.df$Threshold[thresh.ind]==output.df$`Posterior Probability`[post.ind]) & 
       (output.df$`Predicted Class`[post.ind] == 1) &
       (output.df$`Actual Class`[post.ind] == 0)) {
      c.n <- c.n + 1
    }
  }
  output.roc.df$FP[thresh.ind] <- c.n
}

## Calculate False Negative
for(thresh.ind in 1:nrow(output.roc.df)) {
  c.n <- 0
  for(post.ind in 1:nrow(output.df)) {
    if((output.roc.df$Threshold[thresh.ind]==output.df$`Posterior Probability`[post.ind]) & 
       (output.df$`Predicted Class`[post.ind] == 0) &
       (output.df$`Actual Class`[post.ind] == 1)) {
      c.n <- c.n + 1
    }
  }
  output.roc.df$FN[thresh.ind] <- c.n
}

## Calculate True Negative
for(thresh.ind in 1:nrow(output.roc.df)) {
  c.n <- 0
  for(post.ind in 1:nrow(output.df)) {
    if((output.roc.df$Threshold[thresh.ind]==output.df$`Posterior Probability`[post.ind]) & 
       (output.df$`Actual Class`[post.ind] == output.df$`Predicted Class`[post.ind]) &
       (output.df$`Actual Class`[post.ind] == 0)) {
      c.n <- c.n + 1
    }
  }
  output.roc.df$TN[thresh.ind] <- c.n
}

## Calculate True Positive Rate
for(row in 1:nrow(output.roc.df)) {
  output.roc.df$TPR[row] <- output.roc.df$TP[row]/(output.roc.df$TP[row]+output.roc.df$FN[row])
}

## Calculate False Positive Rate
for(row in 1:nrow(output.roc.df)) {
  output.roc.df$FPR[row] <- output.roc.df$FP[row]/(output.roc.df$FP[row]+output.roc.df$TN[row])
}

## Plot the ROC Curve
plot(output.roc.df$FPR, output.roc.df$TPR, ylim = c(0,1))
# abline(0, 1)

## Assign appropriate variables
cm <- table(output.df$`Actual Class`, output.df$`Predicted Class`)
a <- cm[2,2]
b <- cm[1,2]
c <- cm[2,1]
d <- cm[1,1]

## Calculate Precision
p <- a/(a+c)
p

## Calculate Recall
r <- a/(a+b)
r

## Calculate F-Score
f <- ((2*r*p)/(r+p))
f

## Calculate True Positive Rate of total data
tp <- cm[2,2]
fn <- cm[1,2]
fp <- cm[2,1]
tn <- cm[1,1]
tpr <- tp/(tp+fn)
tpr

## Calculate True Negative Rate of total data
tnr <- tn/(fp+tn)
tnr

## Calculate False Positive Rate of total data
fpr <- fp/(fp+tn)
fpr

## Calculate False Negative Rate of total data
fnr <- fn/(tp+fn)
fnr

```

By including a different proximity measure during the implementation of the kNN algorithm, the prediction results should not change significantly. The two proximity measure that are adjustable in the program are the cosine and jaccard similarity functions. Although the relative values differed due to the differing algorithms, the final results were very similar. For k equal to 10, the ROC curve seems good, but looks incomplete because of the lack of data at each threshold and the large amount of True Negative values in the training dataset. It is however difficult to construct a proper ROC curve, since there are not any True Negative values for each threshold after using the kNN algorithm, which can be seen in the “output.roc.df” output file or the confusion matrices shown above. As a result the precision, recall, and f-score values are skewed, since the False Negative and True Negative values essentially equate to zero. However, the True Positive and False Positive values have high amounts, which seems to be an effect of a lack of False Negative and True Negative values from the actual data, rather than an issue with the kNN algorithm. Moreover, the True Positive Rate of the total training data is 0%, since the amount of True Positive values is 0; whereas the True Negative Rate of the total training data is 1%, since the total amount of True Negative values were correctly predicted. Additionally, the True Negative Rate of the total training data is nearly 76%; whereas the False Negative Rate is about 24%.

## Additional Analysis

```{r analysis}
## Education and Class
ggplot(income.test.df, aes(x = education_cat, y = ID, color = class)) + geom_point(aes(size=class))

## Marital Status and Class
lessfif <- subset(income.test.df, income.test.df$class == "<=50K")
grefif <- subset(income.test.df, income.test.df$class == ">50K")
table(lessfif$marital_status)/nrow(lessfif) * 100
table(grefif$marital_status)/nrow(grefif) * 100

## Gender and Class
lessfif <- subset(income.test.df, income.test.df$class == "<=50K")
grefif <- subset(income.test.df, income.test.df$class == ">50K")
table(lessfif$gender)/nrow(lessfif) * 100
table(grefif$gender)/nrow(grefif) * 100
```

After further analysis, there seems to be more reason to establish a decision boundary using the kNN algorithm for the “class” variable. There seem to be a greater proportion of more educated individuals who are of a greater class, which may seem obvious, but a notable trend regardless. Additionally, the proportion of married individuals from the class greater than 50k is nearly 78%; whereas the proportion of married individuals from the class less than 50k is only 41%, which is nearly half of the former figure. Interestingly, nearly 40% of individuals who are a part of the “<=50K class” have never been married; whereas only about 7% of individuals from the class greater than 50k have never been married. Lastly, the proportion of female individuals from the class greater than 50k is only about 18%; whereas the proportion of female individuals from the class less than 50k is nearly 29%, which is nearly a 10% increase from the former figure.

## Export Data Frames

```{r export}
## Export jaccard similarity output - training data
write.csv(jaccard.df, file = "JaccardSimilarityOutputTrain.csv")

## Export jaccard similarity output - testing data
write.csv(jaccard.test.df, file = "JaccardSimilarityOutputTest.csv")

## Export General Output
write.csv(output.df, file = "OutputDataFrame.csv")

## Export Roc Output
write.csv(output.roc.df, file = "OutputROCDataFrame.csv")
```

For the provided output data frame, the classes are transformed binary data. Meaning, "class" variable that is ">50K" is assigned a "1" value; whereas "<=50K" is assigned a "0" value.

